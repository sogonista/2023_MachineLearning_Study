{"cells":[{"cell_type":"markdown","metadata":{"id":"TBFXQGKYUc4X"},"source":["##### Copyright 2018 The TensorFlow Authors."]},{"cell_type":"code","execution_count":12,"metadata":{"cellView":"form","id":"1z4xy2gTUc4a","executionInfo":{"status":"ok","timestamp":1673269886553,"user_tz":-540,"elapsed":973,"user":{"displayName":"sogo takabayashi","userId":"04600456997241825626"}}},"outputs":[],"source":["#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#\n","# https://www.apache.org/licenses/LICENSE-2.0\n","#\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License."]},{"cell_type":"markdown","metadata":{"id":"FE7KNzPPVrVV"},"source":["# Image classification"]},{"cell_type":"markdown","metadata":{"id":"KwQtSOz0VrVX"},"source":["<table class=\"tfo-notebook-buttons\" align=\"left\">\n","  <td>\n","    <a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/images/classification\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n","  </td>\n","  <td>\n","    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/ja/tutorials/images/classification.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n","  </td>\n","  <td>\n","    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/ja/tutorials/images/classification.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n","  </td>\n","  <td>\n","    <a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/ja/tutorials/images/classification.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n","  </td>\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"DjmULc9dAvL-"},"source":["Note: これらのドキュメントは私たちTensorFlowコミュニティが翻訳したものです。コミュニティによる 翻訳は**ベストエフォート**であるため、この翻訳が正確であることや[英語の公式ドキュメント](https://www.tensorflow.org/?hl=en)の 最新の状態を反映したものであることを保証することはできません。 この翻訳の品質を向上させるためのご意見をお持ちの方は、GitHubリポジトリ[tensorflow/docs](https://github.com/tensorflow/docs)にプルリクエストをお送りください。 コミュニティによる翻訳やレビューに参加していただける方は、 [docs-ja@tensorflow.org メーリングリスト](https://groups.google.com/a/tensorflow.org/forum/#!forum/docs-ja)にご連絡ください。"]},{"cell_type":"markdown","metadata":{"id":"gN7G9GFmVrVY"},"source":["このチュートリアルでは、画像から猫または犬を分類する方法を示します。 `tf.keras.Sequential` モデルを使用して画像分類器を構築し、 `tf.keras.preprocessing.image.ImageDataGenerator` を使用してデータをロードします。このチュートリアルでは、以下のコンセプトにしたがって、実践的な経験と感覚を養います。\n","\n","* `tf.keras.preprocessing.image.ImageDataGenerator` クラスを使用して _データ入力パイプライン_ を構築し、モデルで使用するディスク上のデータを効率的に処理します。\n","* _過学習（Overfitting）_ —過学習を識別および防止する方法。\n","* _データ拡張（Data Augmentation）_ および _ドロップアウト（dropout）_ —データパイプラインおよび画像分類モデルに組み込むコンピュータービジョンタスクの過学習と戦うための重要なテクニック。\n","\n","このチュートリアルは、基本的な機械学習のワークフローに従います。\n","\n","1. データの調査及び理解\n","2. 入力パイプラインの構築\n","3. モデルの構築\n","4. モデルの学習\n","5. モデルのテスト\n","6. モデルの改善とプロセスの繰り返し"]},{"cell_type":"markdown","source":["元のURL\n","https://www.tensorflow.org/tutorials/images/classification?hl=ja\n","\n","2022/2/27\n","1個目の改良版\n","pre_normal_r_l.h5の予測モデルが良かったプログラム。"],"metadata":{"id":"MFzhcTpnGr5H"}},{"cell_type":"markdown","metadata":{"id":"zF9uvbXNVrVY"},"source":["## パッケージのインポート"]},{"cell_type":"markdown","metadata":{"id":"VddxeYBEVrVZ"},"source":["まずは必要なパッケージをインポートすることから始めましょう。 `os`パッケージはファイルとディレクトリ構造を読み込み、 NumPy は python リストの numpy 配列への変換と必要な行列演算の実行、 `matplotlib.pyplot` はグラフの描画や学習データおよび検証データに含まれる画像の表示、に利用します。"]},{"cell_type":"markdown","metadata":{"id":"Jlchl4x2VrVg"},"source":["モデルの構築に必要な TensorFlow と Keras クラスをインポートします。"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"E82grprdYPI0","executionInfo":{"status":"ok","timestamp":1673269887479,"user_tz":-540,"elapsed":84,"user":{"displayName":"sogo takabayashi","userId":"04600456997241825626"}}},"outputs":[],"source":["import tensorflow as tf"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"L1WtoaOHVrVh","executionInfo":{"status":"ok","timestamp":1673269887480,"user_tz":-540,"elapsed":84,"user":{"displayName":"sogo takabayashi","userId":"04600456997241825626"}}},"outputs":[],"source":["from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n","\n","\n","\n","import os\n","import numpy as np\n","import matplotlib.pyplot as plt"]},{"cell_type":"markdown","metadata":{"id":"UZZI6lNkVrVm"},"source":["## データの読み込み"]},{"cell_type":"markdown","metadata":{"id":"DPHx8-t-VrVo"},"source":["データセットのダウンロードから始めます。このチュートリアルでは、 Kaggle の <a href=\"https://www.kaggle.com/c/dogs-vs-cats/data\" target=\"_blank\">Dogs vs Cats</a> データセットをフィルタリングしたバージョンを使用します。データセットのアーカイブバージョンをダウンロードし、\"/tmp/\"ディレクトリに保存します。"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"C1nqr-CYY6uw","executionInfo":{"status":"ok","timestamp":1673269887480,"user_tz":-540,"elapsed":82,"user":{"displayName":"sogo takabayashi","userId":"04600456997241825626"}}},"outputs":[],"source":["#ドライブ直接読込\n","PATH = \"/content/drive/MyDrive/Colab Notebooks/cats_and_dogs_filtered\"\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Giv0wMQzVrVw"},"source":["データセットのディレクトリ構造は次のとおりです:\n","\n","<pre>\n","<b>cats_and_dogs_filtered</b>\n","|__ <b>train</b>\n","    |______ <b>cats</b>: [cat.0.jpg, cat.1.jpg, cat.2.jpg ....]  ＞ gamu（ガムシロップ）\n","    |______ <b>dogs</b>: [dog.0.jpg, dog.1.jpg, dog.2.jpg ...]  ＞ milk（ミルク）\n","|__ <b>validation</b>\n","    |______ <b>cats</b>: [cat.2000.jpg, cat.2001.jpg, cat.2002.jpg ....] ＞ gamu（ガムシロップ）\n","    |______ <b>dogs</b>: [dog.2000.jpg, dog.2001.jpg, dog.2002.jpg ...] ＞ milk（ミルク）\n","</pre>"]},{"cell_type":"markdown","metadata":{"id":"VpmywIlsVrVx"},"source":["データの内容を抽出した後、学習および検証セットのための適切なファイルパスで変数を設定します。"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"sRucI3QqVrVy","executionInfo":{"status":"ok","timestamp":1673269887481,"user_tz":-540,"elapsed":82,"user":{"displayName":"sogo takabayashi","userId":"04600456997241825626"}}},"outputs":[],"source":["train_dir = os.path.join(PATH, 'train')\n","validation_dir = os.path.join(PATH, 'validation')"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"Utv3nryxVrV0","executionInfo":{"status":"ok","timestamp":1673269887482,"user_tz":-540,"elapsed":81,"user":{"displayName":"sogo takabayashi","userId":"04600456997241825626"}}},"outputs":[],"source":["train_normal_r_dir = os.path.join(train_dir, 'milk')  # 学習用の猫画像のディレクトリ\n","train_normal_l_dir = os.path.join(train_dir, 'gamu')  # 学習用の犬画像のディレクトリ\n","validation_normal_r_dir = os.path.join(validation_dir, 'milk')  # 検証用の猫画像のディレクトリ\n","validation_normal_l_dir = os.path.join(validation_dir, 'gamu')  # 検証用の犬画像のディレクトリ"]},{"cell_type":"markdown","metadata":{"id":"ZdrHHTy2VrV3"},"source":["### データの理解"]},{"cell_type":"markdown","metadata":{"id":"LblUYjl-VrV3"},"source":["学習および検証ディレクトリの中にある猫と犬の画像の数を見てみましょう:"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"vc4u8e9hVrV4","colab":{"base_uri":"https://localhost:8080/","height":245},"executionInfo":{"status":"error","timestamp":1673269887482,"user_tz":-540,"elapsed":80,"user":{"displayName":"sogo takabayashi","userId":"04600456997241825626"}},"outputId":"3536bc17-d739-4e32-dfce-24ed0088d63a"},"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-18-db394a4d46ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnum_cats_tr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_normal_r_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnum_dogs_tr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_normal_l_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnum_cats_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_normal_r_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnum_dogs_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_normal_l_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/Colab Notebooks/cats_and_dogs_filtered/train/milk'"]}],"source":["num_cats_tr = len(os.listdir(train_normal_r_dir))\n","num_dogs_tr = len(os.listdir(train_normal_l_dir))\n","\n","num_cats_val = len(os.listdir(validation_normal_r_dir))\n","num_dogs_val = len(os.listdir(validation_normal_l_dir))\n","\n","total_train = num_cats_tr + num_dogs_tr\n","total_val = num_cats_val + num_dogs_val"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"g4GGzGt0VrV7","outputId":"1aeebc11-6bdb-41f3-d4fe-df154ee7c09a","colab":{"base_uri":"https://localhost:8080/","height":245},"executionInfo":{"status":"error","timestamp":1673269916888,"user_tz":-540,"elapsed":588,"user":{"displayName":"sogo takabayashi","userId":"04600456997241825626"}}},"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-19-6c2272cf2e83>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'total training normal images:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_cats_tr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'total training glaucoma images:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_dogs_tr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'total validation normal images:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_cats_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'total validation glaucoma images:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_dogs_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'num_cats_tr' is not defined"]}],"source":["print('total training normal images:', num_cats_tr)\n","print('total training glaucoma images:', num_dogs_tr)\n","\n","print('total validation normal images:', num_cats_val)\n","print('total validation glaucoma images:', num_dogs_val)\n","print(\"--\")\n","print(\"Total training images:\", total_train)\n","print(\"Total validation images:\", total_val)"]},{"cell_type":"markdown","metadata":{"id":"8Lp-0ejxOtP1"},"source":["便宜上、データセットの前処理およびネットワークの学習中に使用する変数を設定します。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3NqNselLVrWA","executionInfo":{"status":"aborted","timestamp":1673269887486,"user_tz":-540,"elapsed":78,"user":{"displayName":"sogo takabayashi","userId":"04600456997241825626"}}},"outputs":[],"source":["batch_size = 16\n","epochs = 1000\n","IMG_HEIGHT = 128 #432 #150\n","IMG_WIDTH = 128 #648 #150"]},{"cell_type":"markdown","metadata":{"id":"INn-cOn1VrWC"},"source":["## ***データの準備***"]},{"cell_type":"markdown","metadata":{"id":"5Jfk6aSAVrWD"},"source":["モデルにデータを送る前に、画像を適切に前処理された浮動小数点テンソルにフォーマットします。\n","\n","1.ディスクから画像を読み取ります。\n","2.これらの画像のコンテンツをデコードし、RGB値にしたがって適切なグリッド形式に変換します。\n","3.それらを浮動小数点テンソルに変換します。\n","4.ニューラルネットワークは小さな入力値を扱う方が適しているため、テンソルを0〜255の値から0〜1の値にリスケーリングします。\n","\n","幸い、これらすべてのタスクは、 `tf.keras` によって提供される `ImageDataGenerator` クラスで実行できます。この `ImageDataGenerator` はディスクから画像を読み取り、適切なテンソルに前処理を行います。さらに、これらの画像をテンソルのバッチに変換するジェネレータをセットアップします。これは、ネットワーク学習時に便利です。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"syDdF_LWVrWE","executionInfo":{"status":"aborted","timestamp":1673269887492,"user_tz":-540,"elapsed":86,"user":{"displayName":"sogo takabayashi","userId":"04600456997241825626"}}},"outputs":[],"source":["train_image_generator = ImageDataGenerator(rescale=1./255) # 学習データのジェネレータ\n","validation_image_generator = ImageDataGenerator(rescale=1./255) # 検証データのジェネレータ"]},{"cell_type":"markdown","metadata":{"id":"RLciCR_FVrWH"},"source":["学習および検証画像のジェネレータを定義したのち、 `flow_from_directory` メソッドはディスクから画像をロードし、リスケーリングを適用し、画像を必要な大きさにリサイズします。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pw94ajOOVrWI","executionInfo":{"status":"aborted","timestamp":1673269887499,"user_tz":-540,"elapsed":91,"user":{"displayName":"sogo takabayashi","userId":"04600456997241825626"}}},"outputs":[],"source":["train_data_gen = train_image_generator.flow_from_directory(batch_size=batch_size,\n","                                                           directory=train_dir,\n","                                                           shuffle=True,\n","                                                           target_size=(IMG_HEIGHT, IMG_WIDTH),\n","                                                           class_mode='binary')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2oUoKUzRVrWM","executionInfo":{"status":"aborted","timestamp":1673269887500,"user_tz":-540,"elapsed":91,"user":{"displayName":"sogo takabayashi","userId":"04600456997241825626"}}},"outputs":[],"source":["val_data_gen = validation_image_generator.flow_from_directory(batch_size=batch_size,\n","                                                              directory=validation_dir,\n","                                                              target_size=(IMG_HEIGHT, IMG_WIDTH),\n","                                                              class_mode='binary')"]},{"cell_type":"markdown","metadata":{"id":"hyexPJ8CVrWP"},"source":["### 学習用画像の可視化"]},{"cell_type":"markdown","metadata":{"id":"60CnhEL4VrWQ"},"source":["学習用のジェネレータから画像バッチを抽出して可視化します。（この例では32個の画像を抽出し、そのうち5つを `matplotlib` で描画します。）"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3f0Z7NZgVrWQ","executionInfo":{"status":"aborted","timestamp":1673269887501,"user_tz":-540,"elapsed":90,"user":{"displayName":"sogo takabayashi","userId":"04600456997241825626"}}},"outputs":[],"source":["#sample_training_images, _ = next(train_data_gen)"]},{"cell_type":"markdown","metadata":{"id":"49weMt5YVrWT"},"source":[" `next` 関数はデータセットからバッチを返します。 `next` 関数の返り値は `（x_train、y_train）` の形式で、 `x_train` は学習用の特徴量、 `y_train` はそのラベルです。ラベルを破棄して、学習用画像の可視化のみを行います。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JMt2RES_VrWU","executionInfo":{"status":"aborted","timestamp":1673269887503,"user_tz":-540,"elapsed":91,"user":{"displayName":"sogo takabayashi","userId":"04600456997241825626"}}},"outputs":[],"source":["# この関数は、1行5列のグリッド形式で画像をプロットし、画像は各列に配置されます。\n","#def plotImages(images_arr):\n","#    fig, axes = plt.subplots(1, 5, figsize=(20,20))\n","#    axes = axes.flatten()\n","#    for img, ax in zip( images_arr, axes):\n","#        ax.imshow(img)\n","#        ax.axis('off')\n","#    plt.tight_layout()\n","#    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d_VVg_gEVrWW","executionInfo":{"status":"aborted","timestamp":1673269887505,"user_tz":-540,"elapsed":91,"user":{"displayName":"sogo takabayashi","userId":"04600456997241825626"}}},"outputs":[],"source":["#plotImages(sample_training_images[:5])"]},{"cell_type":"markdown","metadata":{"id":"b5Ej-HLGVrWZ"},"source":["## モデルの構築"]},{"cell_type":"markdown","metadata":{"id":"wEgW4i18VrWZ"},"source":["モデルはmax pooling層を伴う3つの畳み込みブロックからなります。さらに `relu` 活性化関数によるアクティベーションを伴う512ユニットの全結合層があります。モデルは、シグモイド活性化関数による2値分類に基づいてクラスに属する確率を出力します。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F15-uwLPVrWa","executionInfo":{"status":"aborted","timestamp":1673269887508,"user_tz":-540,"elapsed":93,"user":{"displayName":"sogo takabayashi","userId":"04600456997241825626"}}},"outputs":[],"source":["#model = Sequential([\n","#    Conv2D(16, 3, padding='same', activation='relu', input_shape=(IMG_HEIGHT, IMG_WIDTH ,3)),\n","#    MaxPooling2D(),\n","#    Conv2D(32, 3, padding='same', activation='relu'),\n","#    MaxPooling2D(),\n","#    Conv2D(64, 3, padding='same', activation='relu'),\n","#    MaxPooling2D(),\n","#    Flatten(),\n","#    Dense(512, activation='relu'),\n","#    Dense(1, activation='sigmoid')\n","#])"]},{"cell_type":"markdown","metadata":{"id":"PI5cdkMQVrWc"},"source":["### モデルのコンパイル\n","このチュートリアルでは、 *ADAM* オプティマイザーと *binary cross entropy* 損失関数を選択します。各学習エポックの学習と検証の精度を表示するために、`metrics` 引数を渡します。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6Mg7_TXOVrWd","executionInfo":{"status":"aborted","timestamp":1673269887509,"user_tz":-540,"elapsed":94,"user":{"displayName":"sogo takabayashi","userId":"04600456997241825626"}}},"outputs":[],"source":["#model.compile(optimizer='adam',\n","#              loss='binary_crossentropy',\n","#              metrics=['accuracy'])"]},{"cell_type":"markdown","metadata":{"id":"2YmQZ3TAVrWg"},"source":["### モデルの概要\n","\n","すべてのネットワークのレイヤーを見るには、モデルの `summary` メソッドを利用します:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vtny8hmBVrWh","executionInfo":{"status":"aborted","timestamp":1673269887510,"user_tz":-540,"elapsed":95,"user":{"displayName":"sogo takabayashi","userId":"04600456997241825626"}}},"outputs":[],"source":["#model.summary()"]},{"cell_type":"markdown","metadata":{"id":"N06iqE8VVrWj"},"source":["### モデルの学習"]},{"cell_type":"markdown","metadata":{"id":"oub9RtoFVrWk"},"source":["`ImageDataGenerator` クラスの `fit_generator` メソッドを使用して、ネットワークを学習します。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KSF2HqhDVrWk","executionInfo":{"status":"aborted","timestamp":1673269887510,"user_tz":-540,"elapsed":94,"user":{"displayName":"sogo takabayashi","userId":"04600456997241825626"}}},"outputs":[],"source":["#history = model.fit_generator(\n","#    train_data_gen,\n","#    steps_per_epoch=total_train // batch_size,\n","#    epochs=epochs,\n","#    validation_data=val_data_gen,\n","#    validation_steps=total_val // batch_size\n","#\n","#)\n","#\n","#model.save('./pre_normal_r_l.h5')\n"]},{"cell_type":"markdown","metadata":{"id":"ojJNteAGVrWo"},"source":["### 学習結果の可視化"]},{"cell_type":"markdown","metadata":{"id":"LZPYT-EmVrWo"},"source":["ネットワークを学習した後、結果を可視化します。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K6oA77ADVrWp","executionInfo":{"status":"aborted","timestamp":1673269887511,"user_tz":-540,"elapsed":94,"user":{"displayName":"sogo takabayashi","userId":"04600456997241825626"}}},"outputs":[],"source":["#acc = history.history['accuracy']\n","#val_acc = history.history['val_accuracy']\n","#\n","#loss = history.history['loss']\n","#val_loss = history.history['val_loss']\n","#\n","#epochs_range = range(epochs)\n","#\n","#plt.figure(figsize=(8, 8))\n","#plt.subplot(1, 2, 1)\n","#plt.plot(epochs_range, acc, label='Training Accuracy')\n","#plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n","#plt.legend(loc='lower right')\n","#plt.title('Training and Validation Accuracy')\n","#\n","#plt.subplot(1, 2, 2)\n","#plt.plot(epochs_range, loss, label='Training Loss')\n","#plt.plot(epochs_range, val_loss, label='Validation Loss')\n","#plt.legend(loc='upper right')\n","#plt.title('Training and Validation Loss')\n","#plt.show()"]},{"cell_type":"markdown","metadata":{"id":"kDnr50l2VrWu"},"source":["プロットからわかるように、学習セットの精度と検証セットの精度は大幅に外れており、モデルは検証セットで約70％の精度しか達成していません。\n","\n","何がうまくいかなかったかを見て、モデル全体のパフォーマンスを向上してみましょう。"]},{"cell_type":"markdown","metadata":{"id":"rLO7yhLlVrWu"},"source":["## 過学習"]},{"cell_type":"markdown","metadata":{"id":"hNyx3Lp4VrWv"},"source":["上記のプロットでは、学習セットの精度は時間とともに直線的に向上していますが、検証セットの精度は学習プロセスの中で約70％あたりで頭打ちになっています。そして、学習と検証の精度の違いが顕著です。これは *過学習* のサインです。\n","\n","学習サンプルが少ない場合、モデルは学習サンプルに含まれるノイズや不要な詳細から学習してしまい、これによって新しいサンプルに対するモデルの性能に悪影響を与えることがあります。この現象は、過学習として知られています。過学習とは、モデルが新しいデータセットに対して汎化するのが難しい状態をいいます。\n","\n","学習プロセスにおいて過学習に対抗する手段はいくつかあります。このチュートリアルでは、*データ拡張（data Augmentation）* を使用し、さらにモデルに *ドロップアウト（dropout）* を追加します。"]},{"cell_type":"markdown","metadata":{"id":"UOoVpxFwVrWy"},"source":["## データ拡張（Data augmentation）"]},{"cell_type":"markdown","metadata":{"id":"Wn_QLciWVrWy"},"source":["過学習は一般に、学習サンプルが少ない場合に発生します。この問題を解決する方法の1つは、十分な数の学習サンプルが含まれるようにデータセットを拡張することです。データ拡張は、既存の学習サンプルに対してランダムな変換を行い、データセットとして利用できそうな画像を生成するアプローチをとります。このデータ拡張の目的は、学習中にモデルがまったくおなじ画像を2回利用しないようにすることです。これによってモデルをデータのより多くの特徴を利用し、より汎化することができます。\n","\n","`tf.keras` においては、このデータ拡張を `ImageDataGenerator` クラスを使用して実装します。データセットに対するさまざまな変換を指定することによって、学習プロセス中にそれが適用されます。"]},{"cell_type":"markdown","metadata":{"id":"2uJ1G030VrWz"},"source":["### データの拡張と可視化"]},{"cell_type":"markdown","metadata":{"id":"hvX7hHlgVrW0"},"source":["最初に、ランダムな水平反転による拡張をデータセットに適用し、それぞれの画像が変換後にどのように見えるかを確認します。"]},{"cell_type":"markdown","metadata":{"id":"rlVj6VqaVrW0"},"source":["### 水平反転の適用"]},{"cell_type":"markdown","metadata":{"id":"xcdvx4TVVrW1"},"source":[" このデータ拡張を適用するためには、 `ImageDataGenerator` クラスの引数として `horizontal_flip` を渡し、 `True`を設定します。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Bi1_vHyBVrW2","executionInfo":{"status":"aborted","timestamp":1673269887511,"user_tz":-540,"elapsed":93,"user":{"displayName":"sogo takabayashi","userId":"04600456997241825626"}}},"outputs":[],"source":["#image_gen = ImageDataGenerator(rescale=1./255, horizontal_flip=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zvwqmefgVrW3","executionInfo":{"status":"aborted","timestamp":1673269887512,"user_tz":-540,"elapsed":93,"user":{"displayName":"sogo takabayashi","userId":"04600456997241825626"}}},"outputs":[],"source":["#train_data_gen = image_gen.flow_from_directory(batch_size=batch_size,\n","#                                               directory=train_dir,\n","#                                               shuffle=True,\n","#                                               target_size=(IMG_HEIGHT, IMG_WIDTH))"]},{"cell_type":"markdown","metadata":{"id":"zJpRSxJ-VrW7"},"source":["学習サンプルから1つのサンプル画像を取得する作業を5回繰り返して、おなじ画像に5回データ拡張が適用されるようにします。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RrKGd_jjVrW7","executionInfo":{"status":"aborted","timestamp":1673269887512,"user_tz":-540,"elapsed":91,"user":{"displayName":"sogo takabayashi","userId":"04600456997241825626"}}},"outputs":[],"source":["#augmented_images = [train_data_gen[0][0][0] for i in range(5)]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EvBZoQ9xVrW9","executionInfo":{"status":"aborted","timestamp":1673269887512,"user_tz":-540,"elapsed":89,"user":{"displayName":"sogo takabayashi","userId":"04600456997241825626"}}},"outputs":[],"source":["# 上で学習用画像の可視化のために定義、使用されたおなじカスタムプロット関数を再利用する\n","#plotImages(augmented_images)"]},{"cell_type":"markdown","metadata":{"id":"i7n9xcqCVrXB"},"source":["### 画像のランダムな回転"]},{"cell_type":"markdown","metadata":{"id":"qXnwkzFuVrXB"},"source":["回転のデータ拡張を利用して学習用サンプルをランダムに左右45度の範囲で回転させてみましょう。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1zip35pDVrXB","executionInfo":{"status":"aborted","timestamp":1673269887513,"user_tz":-540,"elapsed":89,"user":{"displayName":"sogo takabayashi","userId":"04600456997241825626"}}},"outputs":[],"source":["image_gen = ImageDataGenerator(rescale=1./255, rotation_range=30) #45"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kVoWh4OIVrXD","executionInfo":{"status":"aborted","timestamp":1673269887513,"user_tz":-540,"elapsed":89,"user":{"displayName":"sogo takabayashi","userId":"04600456997241825626"}}},"outputs":[],"source":["train_data_gen = image_gen.flow_from_directory(batch_size=batch_size,\n","                                              directory=train_dir,\n","                                              shuffle=True,\n","                                               target_size=(IMG_HEIGHT, IMG_WIDTH))\n","\n","#augmented_images = [train_data_gen[0][0][0] for i in range(5)]\n","augmented_images = [train_data_gen[0][0][0] for i in range(20)]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wmBx8NhrVrXK","executionInfo":{"status":"aborted","timestamp":1673269887513,"user_tz":-540,"elapsed":88,"user":{"displayName":"sogo takabayashi","userId":"04600456997241825626"}}},"outputs":[],"source":["#plotImages(augmented_images)"]},{"cell_type":"markdown","metadata":{"id":"FOqGPL76VrXM"},"source":["### ズームによるデータ拡張の適用"]},{"cell_type":"markdown","metadata":{"id":"NvqXaD8BVrXN"},"source":["データセットにズームによるデータ拡張を適用して、画像をランダムに最大50％拡大します。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tGNKLa_YVrXR","executionInfo":{"status":"aborted","timestamp":1673269887513,"user_tz":-540,"elapsed":88,"user":{"displayName":"sogo takabayashi","userId":"04600456997241825626"}}},"outputs":[],"source":["image_gen = ImageDataGenerator(rescale=1./255, zoom_range=0.15) #0.5"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VOvTs32FVrXU","executionInfo":{"status":"aborted","timestamp":1673269887514,"user_tz":-540,"elapsed":89,"user":{"displayName":"sogo takabayashi","userId":"04600456997241825626"}}},"outputs":[],"source":["train_data_gen = image_gen.flow_from_directory(batch_size=batch_size,\n","                                               directory=train_dir,\n","                                               shuffle=True,\n","                                               target_size=(IMG_HEIGHT, IMG_WIDTH))\n","\n","#augmented_images = [train_data_gen[0][0][0] for i in range(5)]\n","augmented_images = [train_data_gen[0][0][0] for i in range(20)]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-KQWw8IZVrXZ","executionInfo":{"status":"aborted","timestamp":1673269887514,"user_tz":-540,"elapsed":89,"user":{"displayName":"sogo takabayashi","userId":"04600456997241825626"}}},"outputs":[],"source":["#plotImages(augmented_images)"]},{"cell_type":"markdown","metadata":{"id":"usS13KCNVrXd"},"source":["### すべてのデータ拡張を同時に利用する"]},{"cell_type":"markdown","metadata":{"id":"OC8fIsalVrXd"},"source":["ここまでで紹介したすべてのデータ拡張機能を適用します。ここでは、学習用画像に対して、リスケール、45度の回転、幅シフト、高さシフト、水平反転、ズームを適用しました。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gnr2xujaVrXe","executionInfo":{"status":"aborted","timestamp":1673269887515,"user_tz":-540,"elapsed":90,"user":{"displayName":"sogo takabayashi","userId":"04600456997241825626"}}},"outputs":[],"source":["image_gen_train = ImageDataGenerator(\n","                    rescale=1./255,\n","                    rotation_range=30,#45,\n","                    width_shift_range=.15,\n","                    height_shift_range=.15,\n","                    #horizontal_flip=True,\n","                    zoom_range=0.15 #0.5\n","                    )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K0Efxy7EVrXh","executionInfo":{"status":"aborted","timestamp":1673269887515,"user_tz":-540,"elapsed":89,"user":{"displayName":"sogo takabayashi","userId":"04600456997241825626"}}},"outputs":[],"source":["train_data_gen = image_gen_train.flow_from_directory(batch_size=batch_size,\n","                                                     directory=train_dir,\n","                                                     shuffle=True,\n","                                                     target_size=(IMG_HEIGHT, IMG_WIDTH),\n","                                                     class_mode='binary')"]},{"cell_type":"markdown","metadata":{"id":"AW-pV5awVrXl"},"source":["これらのデータ拡張がデータセットにランダムに適用されたときに、一つの画像に対して5回の個別の適用を行った際にそれぞれどのように見えるかを可視化します。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z2m68eMhVrXm","executionInfo":{"status":"aborted","timestamp":1673269887515,"user_tz":-540,"elapsed":89,"user":{"displayName":"sogo takabayashi","userId":"04600456997241825626"}}},"outputs":[],"source":["#augmented_images = [train_data_gen[0][0][0] for i in range(5)]\n","augmented_images = [train_data_gen[0][0][0] for i in range(20)]\n","#plotImages(augmented_images)"]},{"cell_type":"markdown","metadata":{"id":"J8cUd7FXVrXq"},"source":["### 検証データジェネレータの構築"]},{"cell_type":"markdown","metadata":{"id":"a99fDBt7VrXr"},"source":["一般に、データ拡張は学習サンプルのみに適用します。今回は、 `ImageDataGenerator` を使用して検証画像に対してリスケールのみを実施し、バッチに変換します。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"54x0aNbKVrXr","executionInfo":{"status":"aborted","timestamp":1673269887516,"user_tz":-540,"elapsed":90,"user":{"displayName":"sogo takabayashi","userId":"04600456997241825626"}}},"outputs":[],"source":["image_gen_val = ImageDataGenerator(rescale=1./255)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1PCHKzI8VrXv","executionInfo":{"status":"aborted","timestamp":1673269887516,"user_tz":-540,"elapsed":90,"user":{"displayName":"sogo takabayashi","userId":"04600456997241825626"}}},"outputs":[],"source":["val_data_gen = image_gen_val.flow_from_directory(batch_size=batch_size,\n","                                                 directory=validation_dir,\n","                                                 target_size=(IMG_HEIGHT, IMG_WIDTH),\n","                                                 class_mode='binary')"]},{"cell_type":"markdown","metadata":{"id":"yQGhdqHFVrXx"},"source":["## ドロップアウト（dropout）"]},{"cell_type":"markdown","metadata":{"id":"2Iq5TAH_VrXx"},"source":["過学習を避けるもう一つの方法は、ネットワークに *ドロップアウト* を導入することです。これは、ネットワークにおいて重みを小さくする正則化の方式で、これによって重みの値の分布がより規則的になり、少ない学習データに対する過学習を減らすことができます。ドロップアウトはこのチュートリアルで利用される正則化手法の一つです。\n","\n","ドロップアウトをレイヤーに適用すると、学習プロセス中に適用されたレイヤーのうちランダムに出力ユニットをドロップアウト（ゼロに設定）します。ドロップアウトは、入力値として0.1、0.2、0.4といった形式の小数をとります。これは、適用されたレイヤーからランダムに出力単位の10％、20％、または40％をドロップアウトすることを意味します。\n","\n","特定のレイヤーに0.1ドロップアウトを適用すると、各学習エポックにおいて出力ユニットの10％がランダムに0にされます。\n","\n","この新しいドロップアウト機能を使用したネットワークアーキテクチャを作成し、異なる畳み込みレイヤーや全接続レイヤーに適用してみましょう。"]},{"cell_type":"markdown","metadata":{"id":"DyxxXRmVVrXy"},"source":["## ドロップアウトを追加した新しいネットワークの構築"]},{"cell_type":"markdown","metadata":{"id":"1Ba2LjtkVrXy"},"source":["ここでは、ドロップアウトを最初と最後の max pool 層に適用します。ドロップアウトを適用すると、各学習エポック中にニューロンの20％がランダムにゼロに設定されます。これにより、学習データセットに対する過学習を避けることができます。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2fjio8EsVrXz","executionInfo":{"status":"aborted","timestamp":1673269887517,"user_tz":-540,"elapsed":91,"user":{"displayName":"sogo takabayashi","userId":"04600456997241825626"}}},"outputs":[],"source":["#model_new = Sequential([\n","#    Conv2D(512,3, padding='same', activation='relu', input_shape=(IMG_HEIGHT, IMG_WIDTH ,3)),\n","#    MaxPooling2D(),\n","#    Dropout(0.2),#0.2\n","#    Conv2D(64,3, padding='same', activation='relu'),\n","#    MaxPooling2D(),\n","#    Conv2D(32,3, padding='same', activation='relu'),\n","#    MaxPooling2D(),\n","#    Dropout(0.2),#0.2\n","#    Flatten(),\n","#    Dense(16, activation='relu'),\n","#    Dense(1, activation='sigmoid')\n","#])\n","\n","\n","\n","model_new = Sequential([\n","    Conv2D(16,3, padding='same', activation='relu', input_shape=(IMG_HEIGHT, IMG_WIDTH ,3)),\n","    MaxPooling2D(),\n","    Dropout(0.2),#0.2\n","    Conv2D(32,3, padding='same', activation='relu'),\n","    MaxPooling2D(),\n","    Conv2D(64,3, padding='same', activation='relu'),\n","    MaxPooling2D(),\n","    Dropout(0.2),#0.2\n","    Flatten(),\n","    Dense(512, activation='relu'),\n","    Dense(2, activation='softmax') # 犬と猫を識別するため、２クラス分類のため2を指定 softmaxにする\n","])\n","\n","#クラスを増やす参考URL\n","# https://aiacademy.jp/texts/show/?id=164\n","\n","\n","\n","\n","\n","model_new.summary()\n","\n","#model_new = Sequential([\n","#    Conv2D(16, 3, padding='same', activation='relu', input_shape=(IMG_HEIGHT, IMG_WIDTH ,3)),\n","#    MaxPooling2D(),\n","#    Dropout(0.1),\n","#    Conv2D(32, 3, padding='same', activation='relu'),\n","#    MaxPooling2D(),\n","#    Conv2D(64, 3, padding='same', activation='relu'),\n","#    MaxPooling2D(),\n","#    Dropout(0.2),\n","#    Flatten(),\n","#    Dense(512, activation='relu'),\n","#    Dense(1, activation='sigmoid')\n","#])"]},{"cell_type":"markdown","metadata":{"id":"tpTgIxWAVrX0"},"source":["### モデルのコンパイル"]},{"cell_type":"markdown","metadata":{"id":"1osvc_iTVrX1"},"source":["ネットワークにドロップアウトを導入した後、モデルをコンパイルし、レイヤーの概要を表示します。\n","\n","optimizer参考\n","https://www.codetd.com/ja/article/9966554\n","\n","\n","kerasのConv2D（2次元畳み込み層）について調べてみた\n","https://qiita.com/kenichiro-yamato/items/60affeb7ca9f67c87a17\n","\n","TensorFlowのaccuracyの値が少しも変動しません。\n","https://teratail.com/questions/106957\n","\n","Kerasでval_acc、val_auc、val_lossが同じ値のまま更新されない\n","https://cocoinit23.com/keras-val-acc-val-auc-val-loss-same-value/\n","\n","実務で使えるニューラルネットワークの最適化手法\n","https://acro-engineer.hatenablog.com/entry/2019/12/25/130000#Adam\n","\n","オプティマイザの使用方法\n","https://tensorflow.classcat.com/2016/03/29/keras-optimizers/\n","\n","http://tflearn.org/optimizers/"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OkIJhS-WVrX1","executionInfo":{"status":"aborted","timestamp":1673269887518,"user_tz":-540,"elapsed":91,"user":{"displayName":"sogo takabayashi","userId":"04600456997241825626"}}},"outputs":[],"source":["#オプティマイズ調整\n","\n","\n","#授業より\n","# 乱数のシードを固定\n","#tf.random.set_seed(0)\n","\n","# 定数（学習方法設計時に必要となるもの）\n","#LOSS = 'sparse_categorical_crossentropy'# 損失関数：クロスエントロピー\n","##LOSS = 'binary_crossentropy'\n","##METRICS = ['accuracy'] # 評価関数：正解率\n","##OPTIMIZER = tf.keras.optimizers.RMSprop #Adam #最適化で用いる手法\n","##LEARNING_RATE = 0.00001 # 学習率 # RMSprop(lr=0.00005, decay=1e-6)\n","##DECAY = 1e-6 #RMSprop\n","\n","#BATCH_SIZE = 8 # バッチサイズ（損失関数を一度計算するのに使うデータの数）\n","#EPOCHS = 5000 # エポック数（全てのデータを使った学習を何回繰り返すか）\n","#VERBOSE = 0 # 学習の進捗状況の表示（0, 1または2。 0は表示なし, 2は詳細表示）\n","\n","# 学習方法を定義する\n","##model_new.compile(loss=LOSS, # 損失関数\n","##              #optimizer=OPTIMIZER(learning_rate=LEARNING_RATE), # 最適化手法\n","##              optimizer=OPTIMIZER(learning_rate=LEARNING_RATE,decay=DECAY), # 最適化手法\n","##              metrics=METRICS) # 評価指標 ※リスト形式で設定\n","\t\n","!pip install adabelief-tf==0.2.1\n","\n","from adabelief_tf import AdaBeliefOptimizer\n","\n","#adam = tf.keras.optimizers.Adam(lr=1e-5, beta1=0.99)\n","\n","#OPTIMIZER = tf.keras.optimizers.Adam #RMSprop #最適化で用いる手法\n","#OPTIMIZER = tf.keras.optimizers.Adamax #RMSprop #最適化で用いる手法\n","LEARNING_RATE = 1e-6 # 学習率 # RMSprop(lr=0.00005, decay=1e-6)\n","DECAY = 1e-8 #RMSprop\n","\n","#OPTIMIZER = AdaBeliefOptimizer(learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-14, use_locking=False, name='AdaBelief', amsgrad=False)\n","#OPTIMIZER = AdaBeliefOptimizer(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-14, weight_decay=1e-4, rectify=True, amsgrad=False, sma_threshold=5.0, total_steps=0, warmup_proportion=0.1, min_lr=0.0, name='AdaBeliefOptimizer', print_change_log=True)\n","#eps=1e-16、weight_decay =1e-4、ouple betas=(0.9, 0.999) weight_decouple=True、rectify=True、\n","#ok bt8 OPTIMIZER = AdaBeliefOptimizer(learning_rate=1e-6, epsilon=1e-16, rectify=False)\n","#ok bt32 OPTIMIZER = AdaBeliefOptimizer(learning_rate=1e-6, epsilon=1e-16, rectify=False)\n","\n","#以前はこれでうまくいっていた\n","OPTIMIZER = AdaBeliefOptimizer(learning_rate=1e-6, beta_1=0.9, beta_2=0.999, epsilon=1e-16, weight_decay=1e-5, rectify=True, amsgrad=False, sma_threshold=5.0, total_steps=0, warmup_proportion=0.1, min_lr=0.0, name='AdaBeliefOptimizer', print_change_log=True)\n","\n","#OPTIMIZER = AdaBeliefOptimizer(learning_rate=1e-4,beta_1=0.9, beta_2=0.999, epsilon=1e-16,weight_decay=1e-5, rectify=True, amsgrad=False, sma_threshold=5.0,total_steps=10000,warmup_proportion=0.1,min_lr=1e-6, name='AdaBeliefOptimizer', print_change_log=True)\n","\n","\n","#OPTIMIZER = AdaBeliefOptimizer (lr=1e-3)\n","\n","#model_new.compile(OPTIMIZER(learning_rate=LEARNING_RATE),\n","model_new.compile(OPTIMIZER,\n","                  #OPTIMIZER(learning_rate=LEARNING_RATE,decay=DECAY), # 最適化手法\n","                  loss='binary_crossentropy',\n","                  metrics=['accuracy'])\n","\n","#うまくいっているやつ\n","###model_new.compile(optimizer='adam',loss='binary_crossentropy', metrics=['accuracy'])\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"7KiDshEUVrX6"},"source":["### モデルの学習"]},{"cell_type":"markdown","metadata":{"id":"NFj0oVqVVrX6"},"source":["学習サンプルにデータ拡張を導入し、ネットワークにドロップアウトを追加した後、この新しいネットワークを学習します:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GWxHs_luVrX7","executionInfo":{"status":"aborted","timestamp":1673269887519,"user_tz":-540,"elapsed":92,"user":{"displayName":"sogo takabayashi","userId":"04600456997241825626"}}},"outputs":[],"source":["\n","##BATCH_SIZE = 128 # バッチサイズ（損失関数を一度計算するのに使うデータの数）\n","##EPOCHS = 20 # エポック数（全てのデータを使った学習を何回繰り返すか）\n","#VERBOSE = 0 # 学習の進捗状況の表示（0, 1または2。 0は表示なし, 2は詳細表示）\n","\n","\n","# モデルの学習開始(historyに学習過程のデータを保存)\n","##history = model_new.fit(train_data_gen,\n","##                    batch_size = BATCH_SIZE,\n","##                    epochs = EPOCHS,\n","                   # verbose = VERBOSE,\n","##                    steps_per_epoch=total_train // batch_size,\n","##                    validation_data=val_data_gen,\n","##                    validation_steps=total_val // batch_size\n","##                   )\n","\n","\n","history = model_new.fit_generator(\n","    train_data_gen,\n","    steps_per_epoch=total_train // batch_size,\n","    epochs=epochs,\n","    validation_data=val_data_gen,\n","    validation_steps=total_val // batch_size\n",")\n","\n","#モデルの保存と復元方法\n","#https://www.tensorflow.org/tutorials/keras/save_and_load?hl=ja\n","\n","# HDF5 ファイルにモデル全体を保存\n","# 拡張子 '.h5' はモデルが HDF5 で保存されているということを暗示する\n","#model.save('./my_model.h5')\n","model_new.save('./normal_r_l.h5')\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"bbdyqZdxVrYA"},"source":["### モデルの可視化"]},{"cell_type":"markdown","metadata":{"id":"OgvF2nt7OtR7"},"source":["学習後に新しいモデルを可視化すると、過学習が前回よりも大幅に少ないことがわかります。より多くのエポックでモデルを学習すると、精度はさらに向上するはずです。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7BTeMuNAVrYC","executionInfo":{"status":"aborted","timestamp":1673269887520,"user_tz":-540,"elapsed":93,"user":{"displayName":"sogo takabayashi","userId":"04600456997241825626"}}},"outputs":[],"source":["acc = history.history['accuracy']\n","val_acc = history.history['val_accuracy']\n","\n","loss = history.history['loss']\n","val_loss = history.history['val_loss']\n","\n","epochs_range = range(epochs)\n","\n","plt.figure(figsize=(8, 8))\n","plt.subplot(1, 2, 1)\n","plt.plot(epochs_range, acc, label='Training Accuracy')\n","plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n","plt.legend(loc='lower right')\n","plt.title('Training and Validation Accuracy')\n","\n","plt.subplot(1, 2, 2)\n","plt.plot(epochs_range, loss, label='Training Loss')\n","plt.plot(epochs_range, val_loss, label='Validation Loss')\n","plt.legend(loc='upper right')\n","plt.title('Training and Validation Loss')\n","plt.show()"]},{"cell_type":"code","source":["import PIL.Image\n","import numpy as np\n","import tensorflow_hub as hub\n","jpg = 0\n","if jpg == 0:\n","   image_path     = \"./pred-Gamu1.jpg\"\n","elif jpg == 1:\n","   image_path     = \"./pred-Milk1.jpg\" \n","\n","\n","image = PIL.Image.open(image_path).convert(\"RGB\").resize((128, 128))\n","image = np.array(image) / 255\n","image = np.expand_dims(image, 0)\n","\n","#model_new = tf.keras.models.load_model(\"./my_model.h5\", custom_objects={\"KerasLayer\": hub.KerasLayer})\n","model_pred = tf.keras.models.load_model(\"./normal_r_l.h5\", compile=False , custom_objects={\"KerasLayer\": hub.KerasLayer})\n","#predictions = model_pred.predict(image)\n","print(predictions[0][0])\n","print(predictions)\n","\n","\n","#if predictions == 0:\n","#    print(\">>> 犬\")\n","#elif predictions == 1:\n","#    print(\">>> 猫\")"],"metadata":{"id":"_ry25NRjADAa","executionInfo":{"status":"aborted","timestamp":1673269887521,"user_tz":-540,"elapsed":94,"user":{"displayName":"sogo takabayashi","userId":"04600456997241825626"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["参考URL\n","TensorFlow2 + Keras による画像分類に挑戦4 ～学習済みモデルで予測させてみる～\n","https://qiita.com/code0327/items/1047adc050ab6d75ad5c"],"metadata":{"id":"Fv9jkj4SE0dA"}},{"cell_type":"markdown","source":[],"metadata":{"id":"ONOABE9qrqex"}}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}